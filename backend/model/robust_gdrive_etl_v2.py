import os
import re
import io
import csv
import time
import random
import hashlib
import logging
import threading
import queue
import redis
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from sqlalchemy import create_engine, text
from urllib.parse import quote_plus
from dotenv import load_dotenv

from .normalizer import UniversalNormalizer
from utils.circuit_breaker import CircuitBreaker, CircuitBreakerOpenError

load_dotenv()

# Logging Configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] [%(threadName)s] %(message)s')
logger = logging.getLogger("GDriveETLv4")

# Suppress noisy Google API warnings
logging.getLogger('googleapiclient.discovery_cache').setLevel(logging.ERROR)
import warnings
warnings.filterwarnings("ignore", message="file_cache is only supported with oauth2client<4.0.0")

# Configuration Constants
# Fix 6: Move ROOT_FOLDER_ID to environment variable
ROOT_FOLDER_ID = os.getenv('GDRIVE_ROOT_FOLDER_ID', '1ltTYjekxZsk2CdF20tSk1B2FnRn4119E')
SERVICE_ACCOUNT_FILE = os.path.join(os.path.dirname(__file__), 'honey-bee-digital-d96daf6e6faf.json')
# DB Config
DB_USER = os.getenv('DB_USER')
DB_PASS = quote_plus(os.getenv('DB_PASSWORD_PLAIN') or "")
DB_HOST = os.getenv('DB_HOST')
DB_NAME = os.getenv('DB_NAME')
DB_PORT = os.getenv('DB_PORT', '3306')
DATABASE_URI = f"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}"

class GDriveHighSpeedIngestor:
    def __init__(self):
        self.creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive.readonly'])
        # Producer uses a standard pool since it's threaded, not gevent
        self.engine = create_engine(DATABASE_URI, pool_size=35, max_overflow=25, pool_pre_ping=True, pool_recycle=3600, isolation_level="READ COMMITTED")
        self.table_name = "raw_google_map_drive_data"
        self.task_queue = queue.Queue(maxsize=100)
        self.folder_registry = {}  # Changed to Dict {folder_id: modified_time}
        self.processed_files = {}  # Fix 8: Dict {file_id: file_hash} instead of set
        self.shutdown_event = threading.Event()
        self.scanners_finished = threading.Event()
        self._tls = threading.local()
        self.first_run = True  # Track if we need an initial full scan
        self.page_token = None
        # Circuit breaker for Google Drive API calls
        self.api_breaker = CircuitBreaker(name="gdrive_api")
        
        # Stats & Heartbeat
        self.stats_lock = threading.Lock()
        self.total_scanned_folders = 0
        self.total_skipped_folders = 0
        self.total_dispatched_files = 0

    def shutdown(self):
        """Signal the ingestor to stop."""
        self.shutdown_event.set()

    def get_service(self):
        if not hasattr(self._tls, 'service'):
            self._tls.service = build('drive', 'v3', credentials=self.creds, cache_discovery=False)
        return self._tls.service

    def retry_on_429(func):
        def wrapper(*args, **kwargs):
            for attempt in range(5):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if '429' in str(e) or 'rate limit' in str(e).lower():
                        wait = (2 ** attempt) + random.uniform(0, 1)
                        logger.warning(f"Rate limited. Retrying in {wait:.2f}s...")
                        time.sleep(wait)
                    else: raise
            return func(*args, **kwargs)
        return wrapper

    @staticmethod
    def get_file_hash(file_id, modified_time):
        """Generate a hash for file change detection."""
        return hashlib.md5(f"{file_id}:{modified_time}".encode()).hexdigest()

    def load_registry(self):
        """Load already processed file IDs + hashes, folder timestamps."""
        logger.info("Loading Registry Checkpoints...")
        with self.engine.connect() as conn:
            # Load file hashes and status for high-speed change detection
            files = conn.execute(text("SELECT drive_file_id, file_hash, status FROM file_registry"))
            self.processed_files = {row[0]: {"hash": row[1], "status": row[2]} for row in files}
            
            # Load Folder Registry (ID -> ModifiedTime)
            folders = conn.execute(text("SELECT folder_id, drive_modified_at FROM drive_folder_registry"))
            self.folder_registry = {row[0]: row[1] for row in folders}
            
            # Load Token
            res = conn.execute(text("SELECT meta_value FROM etl_metadata WHERE meta_key='last_change_token'"))
            row = res.fetchone()
            if row: self.page_token = row[0] if row else None

        logger.info(f"Registry loaded: {len(self.processed_files)} files, {len(self.folder_registry)} folders.")

    def save_change_token(self, token):
        with self.engine.begin() as conn:
            conn.execute(text("INSERT INTO etl_metadata (meta_key, meta_value) VALUES ('last_change_token', :token) ON DUPLICATE KEY UPDATE meta_value = :token"), {"token": token})

    def get_changes(self):
        """Fetch all pending changes from Google Drive since last token."""
        service = self.get_service()
        if not self.page_token:
            token_res = service.changes().getStartPageToken().execute()
            self.page_token = token_res.get('startPageToken')
            self.save_change_token(self.page_token)
            return []

        try:
            all_changes = []
            current_token = self.page_token
            while True:
                response = service.changes().list(
                    pageToken=current_token, 
                    spaces='drive', 
                    pageSize=100,
                    fields="nextPageToken, newStartPageToken, changes(fileId, removed, file(id, name, mimeType, parents, modifiedTime))"
                ).execute()
                all_changes.extend(response.get('changes', []))
                if 'nextPageToken' in response:
                    current_token = response['nextPageToken']
                else:
                    self.page_token = response.get('newStartPageToken') or current_token
                    break
            
            return all_changes
        except Exception as e:
            logger.warning(f"Changes API error: {e}. Token might be expired.")
            token_res = service.changes().getStartPageToken().execute()
            self.page_token = token_res.get('startPageToken')
            self.save_change_token(self.page_token)
            return []

    def register_folder(self, folder_id, folder_name, modified_at, csv_count=0):
        try:
            with self.engine.begin() as conn:
                sql = text("""
                    INSERT INTO drive_folder_registry (folder_id, folder_name, drive_modified_at, csv_count, status, scanned_at) 
                    VALUES (:id, :name, :mod, :count, 'DONE', NOW()) 
                    ON DUPLICATE KEY UPDATE 
                        drive_modified_at=VALUES(drive_modified_at), 
                        csv_count=VALUES(csv_count), 
                        status='DONE',
                        scanned_at=NOW()
                """)
                conn.execute(sql, {"id": folder_id, "name": folder_name, "mod": modified_at, "count": csv_count})
            
            # Update In-Memory Cache
            self.folder_registry[folder_id] = modified_at
        except Exception as e:
            logger.error(f"Failed to register folder {folder_name}: {e}")

    @retry_on_429
    def list_files(self, parent_id):
        # Circuit breaker + rate limit protected API call
        def _list():
            return self.get_service().files().list(
                q=f"'{parent_id}' in parents and trashed=false", 
                fields="files(id, name, mimeType, modifiedTime)",
                orderBy="modifiedTime desc",
                pageSize=1000 
            ).execute().get('files', [])
        try:
            return self.api_breaker.call(_list)
        except CircuitBreakerOpenError as e:
            logger.warning(f"Circuit breaker OPEN, skipping list_files for {parent_id}: {e}")
            return []

    def scanner_producer(self, folder_id, folder_name, path=""):
        if self.shutdown_event.is_set(): return
        
        # 1. Fetch items (Already sorted by modifiedTime DESC in API call)
        items = self.list_files(folder_id)
        
        # Separate Folders and Files to guarantee processing order
        folders = [item for item in items if item['mimeType'] == 'application/vnd.google-apps.folder']
        csv_files = [item for item in items if item['name'].lower().endswith('.csv')]
        
        # Process NEWEST CSV FILES first
        folder_skipped = 0
        folder_dispatched = 0
        
        for item in csv_files:
            if self.shutdown_event.is_set(): break
            
            # High-Speed Change Detection (Phase 3) using Cached Registry
            current_hash = self.get_file_hash(item['id'], item.get('modifiedTime', ''))
            cached = self.processed_files.get(item['id'], {})
            existing_hash = cached.get('hash')
            status = cached.get('status')
            
            if status == 'PROCESSED' and existing_hash == current_hash:
                folder_skipped += 1
                continue
                
            # New, Modified, or Partial file -> Dispatch (Phase 2 & 3)
            from tasks.gdrive_task.etl_tasks import process_csv_task
            process_csv_task.delay(
                file_id=item['id'], 
                file_name=item['name'],
                folder_id=folder_id, 
                folder_name=folder_name,
                path=f"{path}/{folder_name}", 
                modified_time=item.get('modifiedTime')
            )
            folder_dispatched += 1

        # Then recursively scan NEWEST SUBFOLDERS (Phase 2)
        for folder in folders:
            if self.shutdown_event.is_set(): break
            self.scanner_producer(folder['id'], folder['name'], f"{path}/{folder_name}")
            
        with self.stats_lock:
            self.total_scanned_folders += 1
            self.total_dispatched_files += folder_dispatched
            
        # Register folder scan as done
        mod_time = items[0].get('modifiedTime') if items else datetime.utcnow().isoformat() + "Z"
        if 'T' in mod_time: 
            mod_time = mod_time.replace('T', ' ').replace('Z', '').split('.')[0]
        self.register_folder(folder_id, folder_name, mod_time, len(csv_files))

    # REMOVED: download_csv, worker_consumer, process_file, commit_batch
    # These are now handled by Celery in tasks/gdrive_task/etl_tasks.py

    def run_pipeline(self):
        start_time = time.time()
        self.load_registry()
        
        logger.info("="*60)
        logger.info(f"üìä [STARTUP SUMMARY]")
        logger.info(f"   - Processed Files in DB: {len(self.processed_files)}")
        logger.info(f"   - Registered Folders:    {len(self.folder_registry)}")
        logger.info("="*60)
        
        self.scanners_finished.clear()
        
        # Reset Stats for new run
        with self.stats_lock:
            self.total_scanned_folders = 0
            self.total_skipped_folders = 0
            self.total_dispatched_files = 0
            
        # 1. INITIAL FULL SCAN (Only on first start)
        if self.first_run:
            # Reset Celery Aggregated Counters in Redis ONLY on first startup
            try:
                r = redis.Redis(host='localhost', port=6379, db=0)
                r.set('celery_files_processed', 0)
                r.set('celery_rows_inserted', 0)
                logger.info("üîÑ Redis Counters Reset (files & rows)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to reset Redis counters: {e}")

            logger.info("üé¨ Initializing GDrive Orchestrator v6.0 (Celery Mode)...")
            top_folders = [f for f in self.list_files(ROOT_FOLDER_ID) if f['mimeType'] == 'application/vnd.google-apps.folder']
            
            # We still use a ThreadPool for SCANNING (iterating folders), but not for processing
            with ThreadPoolExecutor(max_workers=32) as executor:
                futures = [executor.submit(self.scanner_producer, f['id'], f['name'], "ROOT") for f in top_folders]
                for f in as_completed(futures): pass
                self.scanners_finished.set()
            
            # Removed redundant Producer-side stats refresh (handled by Celery now)
            self.first_run = False
            self.save_change_token(self.page_token)
            logger.info("="*60)
            logger.info(f"‚úÖ Initial Scan Complete in {time.time() - start_time:.2f}s")
            logger.info(f"   - Total Folders Scanned: {self.total_scanned_folders}")
            logger.info(f"   - Total Folders Skipped: {self.total_skipped_folders}")
            logger.info(f"   - Total Files Dispatched: {self.total_dispatched_files}")
            logger.info("="*60)
            return

        # 2. REACTIVE TARGETED SYNC
        changes = self.get_changes()
        if not changes:
            logger.info("‚ö° System Idle... No new changes detected.")
            return

        logger.info(f"üöÄ Reactive Trigger! Disptaching {len(changes)} updates to Celery...")
        
        from tasks.gdrive_task.etl_tasks import process_csv_task
        
        folders_to_scan = []

        for c in changes:
            if c.get('removed'): continue
            file = c.get('file', {})
            if not file: continue
            
            # Identify what changed
            if file.get('name', '').lower().endswith('.csv'):
                if file['id'] not in self.processed_files:
                     logger.debug(f"üÜï REACTIVE TASK: {file['name']}")
                     process_csv_task.delay(
                        file_id=file['id'], 
                        file_name=file['name'],
                        folder_id="TARGETED", 
                        folder_name="Reactive",
                        path="REACTIVE", 
                        modified_time=file.get('modifiedTime')
                     )
            elif file.get('mimeType') == 'application/vnd.google-apps.folder':
                folders_to_scan.append(file)
        
        # Deduplicate folders to prevent parallel scans of the same folder
        if folders_to_scan:
            seen_ids = set()
            unique_folders = []
            for f in folders_to_scan:
                if f['id'] not in seen_ids:
                    seen_ids.add(f['id'])
                    unique_folders.append(f)
            folders_to_scan = unique_folders
            logger.info(f"üìÇ Scanning {len(folders_to_scan)} unique reactive folders...")

            with ThreadPoolExecutor(max_workers=8) as executor:
                for f in folders_to_scan:
                    executor.submit(self.scanner_producer, f['id'], f['name'], "REACTIVE")
            
        self.save_change_token(self.page_token)
        logger.info(f"‚ú® Reactive Cycle dispatched in {time.time() - start_time:.2f}s")


class ValidationQualityProcessor:
    """
    üõ°Ô∏è Continuous Validation, Cleaning & Master Sync Layer
    Processes raw_google_map_drive_data -> raw_clean_google_map_data & master_table
    Ensures zero data loss, applying robust validation, normalization, and deduplication.
    """
    def __init__(self, engine, shutdown_event):
        self.engine = engine
        self.shutdown_event = shutdown_event
        self.batch_size = 20000 # üöÄ Turbo: Process 20k rows per cycle (Up from 10k)

    def get_last_processed_id(self):
        try:
            with self.engine.connect() as conn:
                # Priority 1: Check newest log entry for last_id
                res = conn.execute(text("SELECT last_id FROM data_validation_log ORDER BY id DESC LIMIT 1"))
                row = res.fetchone()
                if row and row[0]:
                    return int(row[0])
                
                # Priority 2: Fallback to etl_metadata
                res = conn.execute(text("SELECT meta_value FROM etl_metadata WHERE meta_key='last_processed_id'"))
                row = res.fetchone()
                return int(row[0]) if row and str(row[0]).isdigit() else 0
        except Exception:
            return 0

    def update_last_processed_id(self, last_id):
        try:
            with self.engine.begin() as conn:
                conn.execute(text("""
                    INSERT INTO etl_metadata (meta_key, meta_value) 
                    VALUES ('last_processed_id', :val) 
                    ON DUPLICATE KEY UPDATE meta_value = :val
                """), {"val": str(last_id)})
        except Exception as e:
            logger.error(f"Failed to update last_processed_id: {e}")

    def log_validation_batch(self, summary):
        try:
            with self.engine.begin() as conn:
                conn.execute(text("""
                    INSERT INTO data_validation_log 
                    (total_processed, missing_count, valid_count, duplicate_count, cleaned_count, last_id, timestamp)
                    VALUES (:total, :missing, :valid, :duplicate, :cleaned, :last_id, NOW())
                """), summary)
        except Exception as e:
            logger.error(f"Failed to log validation batch: {e}")

    def is_missing(self, val):
        return val is None or str(val).strip() == ""

    def validate_row(self, row):
        # Mandatory fields for STRUCTURED check (Multilingual Safe)
        mandatory_fields = ["name", "address", "phone_number", "city", "state", "category"]
        missing = [f for f in mandatory_fields if self.is_missing(row.get(f))]
        
        is_structured = len(missing) == 0
        
        # Format checks for VALID check
        invalid_fields = []
        
        # Phone (Phase 7): Numeric only, 8-18 digits
        raw_phone = str(row.get('phone_number', '') or '')
        clean_phone = re.sub(r'\D', '', raw_phone)
        if not (self.is_missing(clean_phone)) and not re.match(r'^\d{8,18}$', clean_phone):
            invalid_fields.append("phone_number")
        elif self.is_missing(clean_phone):
            if "phone_number" not in missing:
                missing.append("phone_number")
            
        # Website: Must contain "." if present
        website = str(row.get('website', '') or '').strip()
        if website and '.' not in website:
            invalid_fields.append("website")
            
        is_valid = len(invalid_fields) == 0 and is_structured
        
        return is_structured, is_valid, missing, invalid_fields, clean_phone

    def check_duplicates_batch(self, signatures, conn):
        """Batch check signatures against the clean table."""
        """
        Fast hash-based duplicate check against raw_clean_google_map_data.
        Uses 4-field signature: (phone, name, address, city)
        """
        if not signatures:
            return set()
        
        results = set()
        sig_list = list(signatures)
        
        for i in range(0, len(sig_list), 5000): # üöÄ Turbo: Larger sub-batches (5k)
            batch = sig_list[i:i+5000]
            conditions = []
            params = {}
            for idx, (phone, name, addr, city) in enumerate(batch):
                conditions.append(f"(phone_number = :p{idx} AND LOWER(TRIM(name)) = :n{idx} AND LOWER(TRIM(COALESCE(address,''))) = :a{idx} AND LOWER(TRIM(COALESCE(city,''))) = :c{idx})")
                params[f"p{idx}"] = phone
                params[f"n{idx}"] = name
                params[f"a{idx}"] = addr
                params[f"c{idx}"] = city
            
            if not conditions:
                continue
                
            query = f"""SELECT LOWER(TRIM(name)) as n, phone_number as p, 
                              LOWER(TRIM(COALESCE(address,''))) as a, 
                              LOWER(TRIM(COALESCE(city,''))) as ct
                       FROM raw_clean_google_map_data 
                       WHERE {' OR '.join(conditions)}"""
            try:
                rows = conn.execute(text(query), params).fetchall()
                for r in rows:
                    results.add((str(r[1]), str(r[0]), str(r[2]), str(r[3])))
            except Exception as e:
                logger.warning(f"Duplicate check sub-batch failed: {e}")
        return results

    def start_pipeline(self):
        """Main loop for the quality assurance and master sync thread."""
        last_id = self.get_last_processed_id()
        logger.info(f"üõ°Ô∏è Data Quality Processor Started from ID: {last_id}")
        
        while not self.shutdown_event.is_set():
            try:
                # Use a SINGLE connection for the entire cycle (fetch + check + write)
                with self.engine.begin() as conn:
                    # READ UNCOMMITTED: prevents locking raw table during read
                    conn.execute(text("SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED"))
                    
                    # 1. Fetch batch from Tier 1 (Raw)
                    rows = conn.execute(text("""
                        SELECT id, name, address, website, phone_number, 
                                reviews_count, reviews_average, category, subcategory, 
                                city, state, area, created_at
                        FROM raw_google_map_drive_data 
                        WHERE id > :last_id 
                        ORDER BY id ASC 
                        LIMIT :limit
                    """), {"last_id": last_id, "limit": self.batch_size}).fetchall()
                
                    if not rows:
                        conn.execute(text("SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ"))
                        if self.shutdown_event.wait(timeout=10):
                            break
                        continue

                    batch_summary = {
                        "total": 0,
                        "missing": 0,
                        "valid": 0,
                        "duplicate": 0,
                        "cleaned": 0
                    }
                    
                    current_max_id = last_id
                    
                    # 2. Process batch in memory with FULL NORMALIZATION
                    clean_data_batch = []
                    master_data_batch = []
                    
                    batch_rows = []
                    signatures = set()
                    
                    for row_obj in rows:
                        # Convert to dict and apply FULL Normalization first (as per user request)
                        raw_row = row_obj._asdict() if hasattr(row_obj, '_asdict') else row_obj._mapping
                        norm_row = UniversalNormalizer.normalize_row_full(dict(raw_row))
                        norm_row['id'] = raw_row['id'] # Preserve original ID for tracking
                        
                        batch_rows.append(norm_row)
                        current_max_id = max(current_max_id, norm_row['id'])
                        
                        # Pre-calculate signatures for bulk duplicate check (Normalized)
                        sig = (norm_row['phone_number'], norm_row['name'].lower(), norm_row['address'].lower(), norm_row['city'].lower())
                        signatures.add(sig)

                    # Bulk Duplicate Check (against Clean Table)
                    existing_sigs = self.check_duplicates_batch(signatures, conn)
                    
                    # Switch back to safe mode for writes
                    conn.execute(text("SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ"))

                    for row in batch_rows:
                        batch_summary["total"] += 1
                        
                        # Apply Validation Logic on Normalized Data
                        is_structured, is_valid, missing_list, invalid_list, clean_phone = self.validate_row(row)
                        
                        sig = (row['phone_number'], row['name'].lower(), row['address'].lower(), row['city'].lower())
                        is_duplicate = sig in existing_sigs if is_structured else False

                        # Determine status
                        status = "VALID"
                        if not is_structured: 
                            status = "MISSING"
                            batch_summary["missing"] += 1
                        elif is_duplicate: 
                            status = "DUPLICATE"
                            batch_summary["duplicate"] += 1
                        elif not is_valid: 
                            status = "INVALID"
                        else:
                            batch_summary["valid"] += 1

                        # 3. Track statistics only (Audit table removed)
                        # No audit insertion per user request

                        # 4. Queue for Clean & Master (If successful OR error-tracking for clean table)
                        if status in ["VALID", "MISSING", "INVALID", "DUPLICATE"]:
                            clean_data_batch.append({
                                "raw_id": row['id'], "name": row['name'], "address": row['address'],
                                "website": row['website'], "phone": row['phone_number'], 
                                "reviews": row.get('reviews_count', 0), "avg": row.get('reviews_average', 0.00),
                                "cat": row['category'], "sub": row['subcategory'], "city": row['city'],
                                "state": row['state'], "area": row['area'], "created": row.get('created_at') or datetime.now(),
                                "val_status": status, 
                                "clean_status": "CLEANED" if status == "VALID" else "FAILED_VALIDATION" if status != "DUPLICATE" else "DUPLICATE_FOUND", 
                                "missing": ",".join(missing_list) if missing_list else None, 
                                "invalid": ",".join(invalid_list) if invalid_list else None, 
                                "duplicate_reason": "Exact match (Phone, Name, Address, City)" if status == "DUPLICATE" else None, 
                                "processed_at": datetime.now()
                            })
                            
                            # ONLY Valid rows go to Master
                            if status == "VALID":
                                master_data_batch.append({
                                    "name": row['name'],
                                    "address": row['address'],
                                    "website": row['website'],
                                    "phone_number": row['phone_number'],
                                    "reviews_count": int(row.get('reviews_count', 0)),
                                    "reviews_avg": float(row.get('reviews_average', 0.00)),
                                    "category": row['category'],
                                    "subcategory": row['subcategory'],
                                    "city": row['city'],
                                    "state": row['state'],
                                    "area": row['area'],
                                    "created_at": row.get('created_at') or datetime.now()
                                })
                                batch_summary["cleaned"] += 1

                    # 4. Execute Batch Writes (Clean & Master ONLY)

                    if clean_data_batch:
                        conn.execute(text("""
                            INSERT IGNORE INTO raw_clean_google_map_data 
                            (raw_id, name, address, website, phone_number, reviews_count, reviews_avg,
                             category, subcategory, city, state, area, created_at,
                             validation_status, cleaning_status, missing_fields, invalid_format_fields, duplicate_reason, processed_at)
                            VALUES (:raw_id, :name, :address, :website, :phone, :reviews, :avg, :cat, :sub, :city, :state, :area, :created,
                                    :val_status, :clean_status, :missing, :invalid, :duplicate_reason, :processed_at)
                        """), clean_data_batch)
                        
                    if master_data_batch:
                        conn.execute(text("""
                            INSERT IGNORE INTO g_map_master_table 
                            (name, address, website, phone_number, reviews_count, reviews_avg, category, subcategory, city, state, area, created_at)
                            VALUES (:name, :address, :website, :phone_number, :reviews_count, :reviews_avg, :category, :subcategory, :city, :state, :area, :created_at)
                        """), master_data_batch)

                # 6. Finalize batch
                batch_summary['last_id'] = current_max_id
                self.update_last_processed_id(current_max_id)
                self.log_validation_batch(batch_summary)
                last_id = current_max_id
                
                logger.info(f"üõ°Ô∏è Quality Cycle Complete: Processed {batch_summary['total']} rows. Last ID: {last_id}")

            except Exception as e:
                msg = str(e)
                if "[parameters:" in msg:
                    msg = msg.split("[parameters:")[0] + " [Parameters hidden]"
                logger.error(f"üõ°Ô∏è Validation Loop Error: {msg.strip()}")
                if self.shutdown_event.wait(timeout=10):
                    break

def get_engine():
    return GDriveHighSpeedIngestor()

def start_background_etl():
    """Started by app.py to orchestrate the scanning."""
    ingestor = get_engine()
    
    # Start Drive Scanner Thread
    def scanner_loop():
        while not ingestor.shutdown_event.is_set():
            try:
                ingestor.run_pipeline()
                logger.info("Scanner sleeping for 30s...")
                if ingestor.shutdown_event.wait(timeout=30):
                    break
            except Exception as e:
                logger.error(f"Scanner Loop error: {e}")
                time.sleep(10)
    
    t_scanner = threading.Thread(target=scanner_loop, name="ScannerThread", daemon=True)
    t_scanner.start()

    # Start Validation & Cleaning Thread
    validator = ValidationQualityProcessor(ingestor.engine, ingestor.shutdown_event)
    t_validator = threading.Thread(target=validator.start_pipeline, name="QualityThread", daemon=True)
    t_validator.start()

    return ingestor

if __name__ == "__main__":
    start_background_etl()
    while True: time.sleep(1)
